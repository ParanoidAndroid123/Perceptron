# -*- coding: utf-8 -*-
"""Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pPIbgxtSteJoiFIFDa-cYkcIPbnc_Cgp
"""

# Dataset used - Banknote Authentication dataset.
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt

import numpy as np 
import pandas as pd 
from sklearn.model_selection import train_test_split

def load_dataset():
  '''
  Helper function to load the digits dataset and split them into train and test set.
  Dataset description - 4 attributes for each instance, labelled as 1 or 0 
  Total of Contains 1372 instances, of which 610 are labelled as 1,
  10% of the dataset is used to testing 
  '''
  data = np.genfromtxt('data_banknote_authentication.txt', delimiter=',')
  X, y = data[:, :4], data[:, 4]
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1)
  return (X_train, X_test, y_train, y_test)

class Perceptron:
  '''
  Perceptron learning algorithm implementation 

  Parameters
  ----------
    weights: Weights which the inputs are multiplied by. Initialized after loading dataset
    bias: Bias of the hypothesis function
    lr: Learning rate, default 0.001
    train_X,  train_Y: Training data and labels respectively 
    val_X, val_y: Validation data and labels respectively
  '''
  def __init__(self):
    self.weights = None
    self.bias = None 
    self.lr = 0.001 
    self.train_X = None
    self.train_y = None 
    self.val_X = None 
    self.val_y = None 
  
  def load_data(self, X, y):
    '''
    Loads the dataset and splits into training and validation set. 
    Also initializes weights and bias to required dimensions 
    '''
    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size = 0.1)
    self.train_X = train_X
    self.train_y = train_y 
    self.val_X = val_X
    self.val_y = val_y
    self.weights = np.ones(self.train_X.shape[1])
    self.bias = 0

  def predict(self, x):
    '''
    Predictions output using a weighted sum of inputs and step function 
    '''
    y_out = np.dot(x, self.weights)+self.bias
    return np.where(y_out > 0, 1, 0) 

  def fit(self, n_iters):
    '''
    Fits the model to the dataset. n_iter specifies number of iterations 
    '''
    for iter in range(n_iters):
      for (x, y) in zip(self.train_X, self.train_y):
        y_pred = self.predict(x) # Make prediction 
        error = y - y_pred # Calculate error in prediction 
        self.weights += self.lr*error*x # Update weight according to learning rate and error 
        self.bias += self.lr*error  # Update bias 
      
      val_predictions = self.predict(self.val_X) # Make predictions on validation set 
      acc = np.sum(val_predictions==self.val_y)/self.val_y.shape[0] # Calculate accuracy on validation set 
      print("Iteration: ", iter, "Accuracy: ", acc)

(train_X, test_x, train_y, test_y) = load_dataset()

model = Perceptron()
model.load_data(train_X, train_y)

model.fit(10)

test_pred = model.predict(test_x)
acc = np.sum(test_pred==test_y)/test_y.shape[0]
print(acc)